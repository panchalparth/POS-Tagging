{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "O1Qmg5yxILAz",
        "colab_type": "code",
        "outputId": "a4ee7657-4fe4-4a35-cebf-d681b845618d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"first0.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/12wCE4-KOGy3UpH929Q0mrjAtBzvnGoov\n",
        "\"\"\"\n",
        "\n",
        "import nltk \n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('brown')\n",
        "import numpy as np\n",
        "import copy\n",
        "import pandas as pd\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from nltk.corpus import brown as corpus\n",
        "\n",
        "tagged_words = [ ] #word with tag\n",
        "all_tags = [ ]  #tags sequence \n",
        "\n",
        "my_set={\"START\",\"END\"}\n",
        "for sent in corpus.tagged_sents(tagset='universal'): # get tagged sentences\n",
        "    tagged_words.append( (\"START\", \"START\") )\n",
        "    all_tags.append(\"START\")\n",
        "    for (word, tag) in sent:\n",
        "        all_tags.append(tag)\n",
        "        tagged_words.append( (tag, word) ) \n",
        "        my_set.add(word)\n",
        "    tagged_words.append( (\"END\", \"END\") )\n",
        "    all_tags.append(\"END\")\n",
        "\n",
        "# print(*map(' '.join,nltk.bigrams(all_tags)),sep=' ,')\n",
        "\n",
        "utagset = [\n",
        "    'START',\n",
        "    'VERB',\n",
        "    'NOUN',\n",
        "    'PRON',\n",
        "    'ADJ',\n",
        "    'ADV',\n",
        "    'ADP',\n",
        "    'CONJ',\n",
        "    'DET',\n",
        "    'NUM',\n",
        "    'PRT',\n",
        "    'X',\n",
        "    '.',\n",
        "    'END'\n",
        "]\n",
        "\n",
        "#calculate transition_prob\n",
        "cfd_tags= nltk.ConditionalFreqDist(nltk.bigrams(all_tags))\n",
        "transition_prob_df = pd.DataFrame(index=utagset, columns=utagset, data=np.zeros((len(utagset), len(utagset))))\n",
        "transition_prob_df\n",
        "rowi,colj=transition_prob_df.shape\n",
        "for i in range(rowi):\n",
        "    for j in range(colj):\n",
        "         transition_prob_df.at[utagset[i],utagset[j] ]=(cfd_tags[utagset[i]][utagset[j]])/all_tags.count(utagset[i])\n",
        "\n",
        "\n",
        "#calculate emission_prob\n",
        "ls_col=list(my_set)\n",
        "cfd_tagwords= nltk.ConditionalFreqDist(tagged_words)\n",
        "cpd_tagwords = nltk.ConditionalProbDist(cfd_tagwords, nltk.MLEProbDist)\n",
        "emission_prob_df = pd.DataFrame(index=utagset, columns=ls_col, data=np.zeros((len(utagset), len(my_set))))\n",
        "rowx,coly=emission_prob_df.shape\n",
        "emission_prob_df\n",
        "for i in tqdm(range(int(rowx))):\n",
        "    for j in range(int(coly)):\n",
        "#         cpd_tagwords['DET'].prob('the') \n",
        "#         val=(cfd_tagwords[utagset[i]][ls_col[j]])/all_tags.count(utagset[i])\n",
        "        emission_prob_df.at[utagset[i],ls_col[j] ]= cpd_tagwords[utagset[i]].prob(ls_col[j]) \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 14/14 [00:11<00:00,  1.61it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "what is example of complex sentence\n",
            "DET\n",
            "VERB\n",
            "NOUN\n",
            "ADP\n",
            "ADJ\n",
            "NOUN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ovmjSz2TP_E5",
        "colab_type": "code",
        "outputId": "c44c9533-76f8-4cab-a284-9b3580368e8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "\t\t\n",
        "#take input sentence from user to test\n",
        "s=input().split()\n",
        "utags=[\n",
        "\t'VERB',\n",
        "    'NOUN',\n",
        "    'PRON',\n",
        "    'ADJ',\n",
        "    'ADV',\n",
        "    'ADP',\n",
        "    'CONJ',\n",
        "    'DET',\n",
        "    'NUM',\n",
        "    'PRT',\n",
        "    'X',\n",
        "    '.']\n",
        "\n",
        "#calculate final probability list by multipying transition_prob and emission_prob across layer \n",
        "big_list=list()\n",
        "best_path=list() #list track previous tag index\n",
        "best_path_val=list()\n",
        "for sidx in range(len(s)):\n",
        "    first_list=list()\n",
        "    sub_path=list()\n",
        "    sub_path_val=list()\n",
        "    for tidx in range(len(utags)):\n",
        "        small_list=list()\n",
        "        if (sidx==0):\n",
        "            p=(transition_prob_df.loc['START'][utags[tidx]])*(emission_prob_df.loc[utags[tidx]][s[sidx]])\n",
        "            first_list.append(p)\n",
        "           \n",
        "        else:\n",
        "            for jdx in range(len(utags)):\n",
        "                p=(big_list[sidx-1][jdx])*(transition_prob_df.loc[utags[jdx]][utags[tidx]])*(emission_prob_df.loc[utags[tidx]][s[sidx]])\n",
        "                small_list.append(p)\n",
        "            sub_path.append(small_list.index(max(small_list)))\n",
        "            sub_path_val.append(max(small_list))\n",
        "    \n",
        "    \n",
        "    if(sidx==0):\n",
        "      best_path_val.append(max(first_list))\n",
        "      big_list.append(first_list)\n",
        "    else:\n",
        "      best_path.append(sub_path)\n",
        "      best_path_val.append(sub_path_val)\n",
        "      big_list.append(sub_path_val)\n",
        "\n",
        "#deepcopy of big_list \n",
        "big_copy_list=copy.deepcopy(big_list)\n",
        "paths=list()\n",
        "\n",
        "#func to check list contain probabilities of possible tag for last word is zero or not\n",
        "def check1(list1): \n",
        "  for x in list1: \n",
        "     if x> 0: \n",
        "          return True \n",
        "  return False\n",
        " \n",
        "\n",
        "#iterative code to find possible subsequence  \n",
        "while(check1(big_copy_list[len(big_copy_list)-1])):\n",
        "\n",
        "  idx=big_copy_list[len(big_copy_list)-1].index(max(big_copy_list[len(big_copy_list)-1]))\n",
        "\n",
        "  max1=max(big_copy_list[len(big_copy_list)-1])\n",
        "\n",
        "  dumpath=list()\n",
        "\n",
        "  dumpath.append(idx)\n",
        "\n",
        "  dumidx=len(best_path)-1\n",
        "  inx=idx\n",
        "  while(dumidx>=0):\n",
        "    inx=best_path[dumidx][inx]\n",
        "    dumpath.append(inx)\n",
        "    dumidx-=1\n",
        "\n",
        "  paths.append(dumpath)\n",
        "\n",
        "  big_copy_list[len(big_copy_list)-1][idx]=0\n",
        "\n",
        "\n",
        "#find best possible sequence by calculation of probabilities\n",
        "max_p=list()\n",
        "for slist in paths:\n",
        "  mul=1\n",
        "  k=len(big_list)-1\n",
        "  for ix in slist:\n",
        "    mul=big_list[k][ix]*mul\n",
        "    k-=1\n",
        "  max_p.append(mul)\n",
        "\n",
        "\n",
        "#take max probability index \n",
        "f_idx=max_p.index(max(max_p))\n",
        "paths[f_idx].reverse() #use max probability index for finding sequence in paths \n",
        "\n",
        "\n",
        "#print tag crosspendence to utag sequence\n",
        "for i in paths[f_idx]:\n",
        "  print(utags[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i am lucky\n",
            "NOUN\n",
            "VERB\n",
            "ADJ\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}